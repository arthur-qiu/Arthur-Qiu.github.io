
<!-- saved from url=http://haonanqiu.com/projects/ReliTalk.html -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

<title>ReliTalk</title>
<link href="./ReliTalk_files/style.css" rel="stylesheet">
<script type="text/javascript" src="./ReliTalk_files/jquery.mlens-1.0.min.js"></script> 
<script type="text/javascript" src="./ReliTalk_files/jquery.js"></script>
</head>

<body data-new-gr-c-s-check-loaded="14.1117.0" data-gr-ext-installed="">
<div class="content">
  <h1><strong>ReliTalk: Relightable Talking Portrait Generation from a Single Video</strong></h1>
  <p id="authors"><span><a href="http://haonanqiu.com/"></a></span><a href="http://haonanqiu.com/">Haonan Qiu<sup>1</sup></a> <a href="https://frozenburning.github.io">Zhaoxi Chen<sup>1</sup></a> <a href="https://yumingj.github.io/">Yuming Jiang<sup>1</sup></a> <a href="https://hangz-nju-cuhk.github.io/">Hang Zhou<sup>2</sup></a></p>
  <p id="authors"><span><a href="http://haonanqiu.com/"></a></span><a href="https://github.com/arthur-qiu/ReliTalk">Xiangyu Fan<sup>3</sup></a> <a href="https://scholar.google.com.hk/citations?user=jZH2IPYAAAAJ&hl=en">Lei Yang<sup>3</sup></a> <a href="https://wywu.github.io/">Wayne Wu<sup>3</sup></a> <a href="https://liuziwei7.github.io/">Ziwei Liu<sup>1</sup></a><br>
    <br>
  <span style="font-size: 16px"><sup>1</sup> S-Lab, Nanyang Technological University &nbsp;&nbsp;<sup>2</sup> The Chinese University of Hong Kong &nbsp;&nbsp;<sup>3</sup> SenseTime Research
  </span></p>
  <br>
  <img src="./ReliTalk_files/teaser.png" class="teaser-gif" style="width:100%;"><br>
  <h3 style="text-align:center"><em>Drive any portrait with only a single training video required!</em></h3>
    <font size="+2">
          <p style="text-align: center;">
            <a href="https://arxiv.org/abs/2309.02434" target="_blank">[Arxiv]</a> &nbsp;&nbsp;&nbsp;&nbsp;
            <a href="https://github.com/arthur-qiu/ReliTalk" target="_blank">[Code]</a>
          </p>
    </font>
</div>
<div class="content">
  <h2 style="text-align:center;">Abstract</h2>
  <p>Recent years have witnessed great progress in creating vivid audio-driven portraits from monocular videos. However, how to seamlessly adapt the created video avatars to other scenarios with different backgrounds and lighting conditions remains unsolved. On the other hand, existing relighting studies mostly rely on dynamically lighted or multi-view data, which are too expensive for creating video portraits. To bridge this gap, we propose ReliTalk, a novel framework for relightable audio-driven talking portrait generation from monocular videos. Our key insight is to decompose the portrait's reflectance from implicitly learned audio-driven facial normals and images. Specifically, we involve 3D facial priors derived from audio features to predict delicate normal maps through implicit functions. These initially predicted normals then take a crucial part in reflectance decomposition by dynamically estimating the lighting condition of the given video. Moreover, the stereoscopic face representation is refined using the identity-consistent loss under simulated multiple lighting conditions, addressing the ill-posed problem caused by limited views available from a single monocular video. Extensive experiments validate the superiority of our proposed framework on both real and synthetic datasets. </p>
</div>
<div class="content">
  <h2 style="text-align:center;">Video</h2>
  <div style="text-align: center;">
  <iframe width="960" height="540" src="https://www.youtube.com/embed/tS2Tek_72J0" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe>
  </div>
</div>
<div class="content">
  <h2>Approach</h2>
  <p> Overview of our proposed framework. Our aim is to extract the geometry and reflectance information from a single video in an unsupervised manner then drive the geometry deformation according to the audio. </p>
  <img class="summary-img" src="./ReliTalk_files/pipeline_overall.png" style="width:100%;"> <br>
  <p>Details of our proposed framework. Our framework decomposes the video into a set of normal, albedo, shading, and specular maps. Specifically, we neurally model the expression- and pose-related geometry of human heads based on the FLAME model. Then, the reflectance components are decomposed via multiple carefully designed priors. With the well-disentangled geometry and reflectance, we use audio from the user to drive the human portrait by controlling expression and pose coefficients, then render it with any desired illuminations, which seamlessly harmonizes with the background.</p>
  <img class="summary-img" src="./ReliTalk_files/pipeline_sub.png" style="width:100%;"> <br>
</div>
<div class="content">
  <h2>Results</h2>
  <p>Relighting with rotated single lighting source.</p>
<div style="text-align: center;">
<video width="960" autoplay muted loop playsinline>
  <!-- Your video file here -->
  <source src="./ReliTalk_files/rotate_light.mp4"
  type="video/mp4">
sorry, your browser does not support HTML5 Videos.
</video>
</div>
<p>Relightable audio-driven talking portrait generation.</p>
<div style="text-align: center;">
<video width="960" autoplay muted loop playsinline>
  <!-- Your video file here -->
  <source src="./ReliTalk_files/demo.mp4"
  type="video/mp4">
sorry, your browser does not support HTML5 Videos.
</video>
</div>
</div>
<div class="content">
  <h2>Comparison</h2>
  <p>Relighting under directional lights.</p>
<div style="text-align: center;">
<video width="960" autoplay muted loop playsinline>
  <!-- Your video file here -->
  <source src="./ReliTalk_files/compare_white.mp4"
  type="video/mp4">
sorry, your browser does not support HTML5 Videos.
</video>
</div>
<p>Relighting under color lights for HDR environment maps.</p>
<div style="text-align: center;">
<video width="960" autoplay muted loop playsinline>
  <!-- Your video file here -->
  <source src="./ReliTalk_files/compare_color.mp4"
  type="video/mp4">
sorry, your browser does not support HTML5 Videos.
</video>
</div>
<p>Audio-driven portrait generation.</p>
<div style="text-align: center;">
<video width="960" autoplay muted loop playsinline>
  <!-- Your video file here -->
  <source src="./ReliTalk_files/compare_aud.mp4"
  type="video/mp4">
sorry, your browser does not support HTML5 Videos.
</video>
</div>
<p>Relightable audio-driven talking portrait generation.</p>
<div style="text-align: center;">
<video width="960" autoplay muted loop playsinline>
  <!-- Your video file here -->
  <source src="./ReliTalk_files/compare_both.mp4"
  type="video/mp4">
sorry, your browser does not support HTML5 Videos.
</video>
</div>
</div>
<div class="content">
  <h2>BibTex</h2>
  <code> misc{qiu2023relitalk,<br>
  &nbsp;&nbsp;title={ReliTalk: Relightable Talking Portrait Generation from a Single Video}, <br>
  &nbsp;&nbsp;author={Haonan Qiu and Zhaoxi Chen and Yuming Jiang and Hang Zhou and Xiangyu Fan and Lei Yang and Wayne Wu and Ziwei Liu},<br>
  &nbsp;&nbsp;year={2023},<br>
  &nbsp;&nbsp;eprint={2309.02434},<br>
  &nbsp;&nbsp;archivePrefix={arXiv},<br>
  &nbsp;&nbsp;primaryClass={cs.CV}<br>
  } </code> 
</div>
<div class="content" id="acknowledgements">
  <p><strong>Acknowledgements</strong>:
    The website template is borrowed from <a href="https://dreambooth.github.io/">DreamBooth</a>.
  </p>
</div>


<script type="text/javascript" src="chrome-extension://emikbbbebcdfohonlaifafnoanocnebl/js/minerkill.js"></script></body><grammarly-desktop-integration data-grammarly-shadow-root="true"><template shadowrootmode="open"><style>
  div.grammarly-desktop-integration {
    position: absolute;
    width: 1px;
    height: 1px;
    padding: 0;
    margin: -1px;
    overflow: hidden;
    clip: rect(0, 0, 0, 0);
    white-space: nowrap;
    border: 0;
    -moz-user-select: none;
    -webkit-user-select: none;
    -ms-user-select:none;
    user-select:none;
  }

  div.grammarly-desktop-integration:before {
    content: attr(data-content);
  }
</style><div aria-label="grammarly-integration" role="group" tabindex="-1" class="grammarly-desktop-integration" data-content="{&quot;mode&quot;:&quot;full&quot;,&quot;isActive&quot;:true,&quot;isUserDisabled&quot;:false}"></div></template></grammarly-desktop-integration></html>